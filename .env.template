# Gold Standard Environment Template
# Copy to .env and fill with your credentials. Do NOT commit .env with real keys.

# ══════════════════════════════════════════════════════════════════════════════
# LLM PROVIDER CONFIGURATION
# ══════════════════════════════════════════════════════════════════════════════
# Gold Standard supports multiple LLM backends with automatic fallback:
#   1. Gemini (cloud) - Google's API, high quality, requires API key
#   2. Ollama (local server) - Easy model management, GPU acceleration
#   3. Local LLM (llama.cpp) - On-device inference, no server needed

# LLM Provider Selection
# LLM_PROVIDER=         # Force provider: gemini, ollama, local (default: auto-fallback)
# PREFER_LOCAL_LLM=0    # Set to 1 to use local LLM first, skip cloud

# ──────────────────────────────────────────────────────────────────────────────
# GEMINI (Cloud)
# ──────────────────────────────────────────────────────────────────────────────
# Get API key from: https://ai.google.dev/
GEMINI_API_KEY=your_gemini_api_key_here
# GEMINI_MODEL=models/gemini-2.0-flash    # Model to use (default: gemini-2.0-flash)

# ──────────────────────────────────────────────────────────────────────────────
# OLLAMA (Local Server)
# ──────────────────────────────────────────────────────────────────────────────
# Install: https://ollama.ai  |  Start: ollama serve  |  Pull: ollama pull <model>
# OLLAMA_HOST=http://localhost:11434    # Ollama server URL
OLLAMA_MODEL=Artifact_Virtual/RAEGEN    # Model name (run: ollama list)

# ──────────────────────────────────────────────────────────────────────────────
# LOCAL LLM (llama.cpp / GGUF Models)
# ──────────────────────────────────────────────────────────────────────────────
# Models auto-discovered from: ~/.cache/gold_standard/models/, ./models/
# Download GGUF models from: https://huggingface.co/TheBloke
LOCAL_LLM_MODEL=~/.cache/gold_standard/models/mistral-7b-instruct-v0.2.Q4_K_M.gguf

# GPU Acceleration (requires CUDA/Metal build of llama-cpp-python)
# LOCAL_LLM_GPU_LAYERS=0     # 0=CPU only, -1=all layers to GPU, N=specific layers
# LOCAL_LLM_CONTEXT=4096     # Context window size
# LOCAL_LLM_THREADS=0        # CPU threads (0=auto)
# LOCAL_LLM_AUTO_DOWNLOAD=0  # Set to 1 to auto-download model if none found

# ══════════════════════════════════════════════════════════════════════════════
# INTEGRATIONS
# ══════════════════════════════════════════════════════════════════════════════

# Notion Integration
NOTION_API_KEY=your_notion_api_key_here
NOTION_DATABASE_ID=your_notion_database_id_here

# Image Hosting for Notion Charts (free: https://api.imgbb.com/)
IMGBB_API_KEY=your_imgbb_api_key_here

# Discord (Optional)
# DISCORD_WEBHOOK_URL=

# ══════════════════════════════════════════════════════════════════════════════
# RUNTIME OPTIONS
# ══════════════════════════════════════════════════════════════════════════════
# RUN_INTERVAL_HOURS=4
# LOG_LEVEL=INFO
