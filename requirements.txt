# =============================================================================
# Syndicate - Production Dependencies
# =============================================================================
# Python Version: 3.12
# Install: pip install -r requirements.txt
# Update:  pip install --upgrade -r requirements.txt
# =============================================================================

# -----------------------------------------------------------------------------
# Core Data & Analysis
# -----------------------------------------------------------------------------
yfinance>=0.2.66              # Yahoo Finance market data API
pandas>=2.0.0                 # Data manipulation and analysis
           # Technical analysis indicators (requires numba)
mplfinance>=0.12.10b0         # Financial charting library

# -----------------------------------------------------------------------------
# AI & Machine Learning
# -----------------------------------------------------------------------------
google-genai>=1.55.0    # New Google GenAI client (recommended; replaces google-generativeai)

requests>=2.31.0              # HTTP client for Ollama API

# -----------------------------------------------------------------------------
# Local LLM (Optional - for offline/fallback AI)
# -----------------------------------------------------------------------------
# llama-cpp-python>=0.3.0     # Local LLM inference (GGUF models)
#
# Installation options:
#   CPU only:    pip install llama-cpp-python
#   CUDA GPU:    pip install llama-cpp-python --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu124
#   Metal GPU:   pip install llama-cpp-python (auto-detected on macOS)
#
# Environment variables:
#   LOCAL_LLM_MODEL=path/to/model.gguf    # Path to GGUF model
#   LOCAL_LLM_GPU_LAYERS=-1               # GPU offload (-1=all, 0=CPU only)
#   LOCAL_LLM_AUTO_DOWNLOAD=1             # Auto-download default model
#   LOCAL_LLM_CONTEXT=4096                # Context window size
#   PREFER_LOCAL_LLM=1                    # Use local LLM as primary
#
# Ollama (Alternative local LLM - server-based):
#   Install from: https://ollama.ai
#   Start server: ollama serve
#   Pull models:  ollama pull llama3.2
#   Configure:    OLLAMA_MODEL=llama3.2

# -----------------------------------------------------------------------------
# Utilities & Runtime
# -----------------------------------------------------------------------------
schedule>=1.2.0               # Job scheduling for recurring tasks
colorama>=0.4.6               # Cross-platform colored terminal output
filelock>=3.0.0               # File-based locking for concurrent access
python-dotenv>=1.0.0          # Environment variable management from .env files
tqdm>=4.66.0                  # Progress bars for downloads
prometheus_client>=0.14.1     # Prometheus metrics client

# -----------------------------------------------------------------------------
# Testing (included for CI/CD compatibility)
# -----------------------------------------------------------------------------
pytest>=7.0                   # Testing framework

# Notion and Image Hosting
notion-client>=0.7.1
imgbbpy==0.1.3
notion-client

# Discord client for ops bot
discord.py>=2.3.0

# Web UI
Flask>=3.0.0
Flask-SocketIO>=5.3.0
python-socketio>=5.10.0
eventlet>=0.35.0
